{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pairwise Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.dirname((os.path.abspath(\"\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/tim/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/tim/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/tim/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/tim/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "from src.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(\n",
    "    collection='data/processed/30_5000_1000_collection.pkl',\n",
    "    queries='data/processed/30_5000_1000_queries.pkl',\n",
    "    queries_val='data/processed/30_5000_1000_queries_val.pkl',\n",
    "    queries_test='data/processed/30_5000_1000_queries_test.pkl',\n",
    "    features='data/processed/30_5000_1000_features.pkl',\n",
    "    qrels_val='data/processed/30_5000_1000_qrels_val.pkl',\n",
    "    qrels_test='data/processed/30_5000_1000_qrels_test.pkl',\n",
    "    features_test='data/processed/30_5000_1000_features_test.pkl',\n",
    "    features_val='data/processed/30_5000_1000_features_val.pkl',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qID</th>\n",
       "      <th>pID</th>\n",
       "      <th>y</th>\n",
       "      <th>w2v_cosine</th>\n",
       "      <th>w2v_euclidean</th>\n",
       "      <th>w2v_manhattan</th>\n",
       "      <th>w2v_tfidf_cosine</th>\n",
       "      <th>w2v_tfidf_euclidean</th>\n",
       "      <th>w2v_tfidf_manhattan</th>\n",
       "      <th>tfidf_cosine</th>\n",
       "      <th>...</th>\n",
       "      <th>polarity_doc</th>\n",
       "      <th>subjectivity_query</th>\n",
       "      <th>polarity_query</th>\n",
       "      <th>bm25</th>\n",
       "      <th>doc_nouns</th>\n",
       "      <th>doc_adjectives</th>\n",
       "      <th>doc_verbs</th>\n",
       "      <th>query_nouns</th>\n",
       "      <th>query_adjectives</th>\n",
       "      <th>query_verbs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>603195</td>\n",
       "      <td>7050012</td>\n",
       "      <td>1</td>\n",
       "      <td>0.972107</td>\n",
       "      <td>144.641830</td>\n",
       "      <td>1124.871630</td>\n",
       "      <td>0.938781</td>\n",
       "      <td>2.765727</td>\n",
       "      <td>22.236694</td>\n",
       "      <td>0.537439</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-24.655536</td>\n",
       "      <td>23</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>474183</td>\n",
       "      <td>325505</td>\n",
       "      <td>1</td>\n",
       "      <td>0.971866</td>\n",
       "      <td>131.960266</td>\n",
       "      <td>1033.670312</td>\n",
       "      <td>0.985675</td>\n",
       "      <td>1.360485</td>\n",
       "      <td>11.347487</td>\n",
       "      <td>0.745907</td>\n",
       "      <td>...</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-33.129796</td>\n",
       "      <td>18</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>320545</td>\n",
       "      <td>1751825</td>\n",
       "      <td>1</td>\n",
       "      <td>0.947701</td>\n",
       "      <td>94.900002</td>\n",
       "      <td>756.378183</td>\n",
       "      <td>0.959522</td>\n",
       "      <td>2.236971</td>\n",
       "      <td>17.352688</td>\n",
       "      <td>0.409509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-16.699603</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>89798</td>\n",
       "      <td>5069949</td>\n",
       "      <td>1</td>\n",
       "      <td>0.972710</td>\n",
       "      <td>161.470459</td>\n",
       "      <td>1273.643564</td>\n",
       "      <td>0.933304</td>\n",
       "      <td>1.714253</td>\n",
       "      <td>13.493497</td>\n",
       "      <td>0.541627</td>\n",
       "      <td>...</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-27.678576</td>\n",
       "      <td>25</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1054603</td>\n",
       "      <td>2869106</td>\n",
       "      <td>1</td>\n",
       "      <td>0.965680</td>\n",
       "      <td>155.648453</td>\n",
       "      <td>1216.564726</td>\n",
       "      <td>0.941391</td>\n",
       "      <td>1.799412</td>\n",
       "      <td>14.369308</td>\n",
       "      <td>0.438115</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-28.497519</td>\n",
       "      <td>20</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>128401</td>\n",
       "      <td>6127598</td>\n",
       "      <td>0</td>\n",
       "      <td>0.796978</td>\n",
       "      <td>85.670822</td>\n",
       "      <td>678.466760</td>\n",
       "      <td>0.555981</td>\n",
       "      <td>3.027138</td>\n",
       "      <td>24.841764</td>\n",
       "      <td>0.185056</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.520833</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-8.866170</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>1044540</td>\n",
       "      <td>4616118</td>\n",
       "      <td>0</td>\n",
       "      <td>0.922095</td>\n",
       "      <td>157.044754</td>\n",
       "      <td>1238.354322</td>\n",
       "      <td>0.603788</td>\n",
       "      <td>2.167866</td>\n",
       "      <td>17.812756</td>\n",
       "      <td>0.140057</td>\n",
       "      <td>...</td>\n",
       "      <td>0.156250</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-7.852468</td>\n",
       "      <td>25</td>\n",
       "      <td>9</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>486146</td>\n",
       "      <td>1137390</td>\n",
       "      <td>0</td>\n",
       "      <td>0.946438</td>\n",
       "      <td>125.126984</td>\n",
       "      <td>972.330644</td>\n",
       "      <td>0.882998</td>\n",
       "      <td>4.161341</td>\n",
       "      <td>34.815641</td>\n",
       "      <td>0.314505</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-15.909103</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>532697</td>\n",
       "      <td>5161847</td>\n",
       "      <td>0</td>\n",
       "      <td>0.938939</td>\n",
       "      <td>99.808395</td>\n",
       "      <td>790.453814</td>\n",
       "      <td>0.893834</td>\n",
       "      <td>1.977307</td>\n",
       "      <td>16.122506</td>\n",
       "      <td>0.344173</td>\n",
       "      <td>...</td>\n",
       "      <td>0.284375</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-16.617979</td>\n",
       "      <td>18</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>763472</td>\n",
       "      <td>6447198</td>\n",
       "      <td>0</td>\n",
       "      <td>0.988622</td>\n",
       "      <td>134.510406</td>\n",
       "      <td>1094.769867</td>\n",
       "      <td>0.977100</td>\n",
       "      <td>1.039390</td>\n",
       "      <td>8.350440</td>\n",
       "      <td>0.290261</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.15</td>\n",
       "      <td>-16.757101</td>\n",
       "      <td>12</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9977 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          qID      pID  y  w2v_cosine  w2v_euclidean  w2v_manhattan  \\\n",
       "0      603195  7050012  1    0.972107     144.641830    1124.871630   \n",
       "1      474183   325505  1    0.971866     131.960266    1033.670312   \n",
       "2      320545  1751825  1    0.947701      94.900002     756.378183   \n",
       "3       89798  5069949  1    0.972710     161.470459    1273.643564   \n",
       "4     1054603  2869106  1    0.965680     155.648453    1216.564726   \n",
       "...       ...      ... ..         ...            ...            ...   \n",
       "4995   128401  6127598  0    0.796978      85.670822     678.466760   \n",
       "4996  1044540  4616118  0    0.922095     157.044754    1238.354322   \n",
       "4997   486146  1137390  0    0.946438     125.126984     972.330644   \n",
       "4998   532697  5161847  0    0.938939      99.808395     790.453814   \n",
       "4999   763472  6447198  0    0.988622     134.510406    1094.769867   \n",
       "\n",
       "      w2v_tfidf_cosine  w2v_tfidf_euclidean  w2v_tfidf_manhattan  \\\n",
       "0             0.938781             2.765727            22.236694   \n",
       "1             0.985675             1.360485            11.347487   \n",
       "2             0.959522             2.236971            17.352688   \n",
       "3             0.933304             1.714253            13.493497   \n",
       "4             0.941391             1.799412            14.369308   \n",
       "...                ...                  ...                  ...   \n",
       "4995          0.555981             3.027138            24.841764   \n",
       "4996          0.603788             2.167866            17.812756   \n",
       "4997          0.882998             4.161341            34.815641   \n",
       "4998          0.893834             1.977307            16.122506   \n",
       "4999          0.977100             1.039390             8.350440   \n",
       "\n",
       "      tfidf_cosine  ...  polarity_doc  subjectivity_query  polarity_query  \\\n",
       "0         0.537439  ...      0.000000                0.00            0.00   \n",
       "1         0.745907  ...      0.450000                0.00            0.00   \n",
       "2         0.409509  ...      0.500000                0.20            0.20   \n",
       "3         0.541627  ...      0.066667                0.25            0.00   \n",
       "4         0.438115  ...      0.000000                0.00            0.00   \n",
       "...            ...  ...           ...                 ...             ...   \n",
       "4995      0.185056  ...     -0.520833                0.00            0.00   \n",
       "4996      0.140057  ...      0.156250                0.00            0.00   \n",
       "4997      0.314505  ...     -0.100000                0.10            0.00   \n",
       "4998      0.344173  ...      0.284375                0.00            0.00   \n",
       "4999      0.290261  ...      0.033333                0.05            0.15   \n",
       "\n",
       "           bm25  doc_nouns  doc_adjectives  doc_verbs  query_nouns  \\\n",
       "0    -24.655536         23               6          4            3   \n",
       "1    -33.129796         18               9          3            4   \n",
       "2    -16.699603         20               2         14            2   \n",
       "3    -27.678576         25              10          5            3   \n",
       "4    -28.497519         20               9          6            2   \n",
       "...         ...        ...             ...        ...          ...   \n",
       "4995  -8.866170         16               6         13            2   \n",
       "4996  -7.852468         25               9         16            0   \n",
       "4997 -15.909103         12               1         10            2   \n",
       "4998 -16.617979         18               8          9            3   \n",
       "4999 -16.757101         12              16          4            1   \n",
       "\n",
       "      query_adjectives  query_verbs  \n",
       "0                    1            1  \n",
       "1                    0            0  \n",
       "2                    1            1  \n",
       "3                    1            0  \n",
       "4                    2            1  \n",
       "...                ...          ...  \n",
       "4995                 1            0  \n",
       "4996                 0            1  \n",
       "4997                 0            2  \n",
       "4998                 1            0  \n",
       "4999                 3            1  \n",
       "\n",
       "[9977 rows x 38 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RankNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Naive Bayes Combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/40    Batch: 1801   Batch loss: 0.026186   \n",
      "Epoch: 10/40    Batch: 1802   Batch loss: 0.047690   \n",
      "Epoch: 10/40    Batch: 1803   Batch loss: 0.011433   \n",
      "Epoch: 10/40    Batch: 1804   Batch loss: 0.013142   \n",
      "Epoch: 10/40    Batch: 1805   Batch loss: 0.011464   \n",
      "Epoch: 10/40    Batch: 1806   Batch loss: 0.003597   \n",
      "Epoch: 10/40    Batch: 1807   Batch loss: 0.011469   \n",
      "Epoch: 10/40    Batch: 1808   Batch loss: 0.134327   \n",
      "Epoch: 10/40    Batch: 1809   Batch loss: 0.029781   \n",
      "Epoch: 10/40    Batch: 1810   Batch loss: 0.017538   \n",
      "Epoch: 10/40    Batch: 1811   Batch loss: 0.058043   \n",
      "Epoch: 10/40    Batch: 1812   Batch loss: 0.039997   \n",
      "Epoch: 10/40    Batch: 1813   Batch loss: 0.010408   \n",
      "Epoch: 10/40    Batch: 1814   Batch loss: 0.057322   \n",
      "Epoch: 10/40    Batch: 1815   Batch loss: 0.008111   \n",
      "Epoch: 10/40    Batch: 1816   Batch loss: 0.045464   \n",
      "Epoch: 10/40    Batch: 1817   Batch loss: 0.024640   \n",
      "Epoch: 10/40    Batch: 1818   Batch loss: 0.008803   \n",
      "Epoch: 10/40    Batch: 1819   Batch loss: 0.019350   \n",
      "Epoch: 10/40    Batch: 1820   Batch loss: 0.105641   \n",
      "Epoch: 10/40    Batch: 1821   Batch loss: 0.072761   \n",
      "Epoch: 10/40    Batch: 1822   Batch loss: 0.097051   \n",
      "Epoch: 10/40    Batch: 1823   Batch loss: 0.005402   \n",
      "Epoch: 10/40    Batch: 1824   Batch loss: 0.178737   \n",
      "Epoch: 10/40    Batch: 1825   Batch loss: 0.006957   \n",
      "Epoch: 10/40    Batch: 1826   Batch loss: 0.014447   \n",
      "Epoch: 10/40    Batch: 1827   Batch loss: 0.057231   \n",
      "Epoch: 10/40    Batch: 1828   Batch loss: 0.047900   \n",
      "Epoch: 10/40    Batch: 1829   Batch loss: 0.070424   \n",
      "Epoch: 10/40    Batch: 1830   Batch loss: 0.031813   \n",
      "Epoch: 10/40    Batch: 1831   Batch loss: 0.081955   \n",
      "Epoch: 10/40    Batch: 1832   Batch loss: 0.014156   \n",
      "Epoch: 10/40    Batch: 1833   Batch loss: 0.004396   \n",
      "Epoch: 10/40    Batch: 1834   Batch loss: 0.025219   \n",
      "Epoch: 10/40    Batch: 1835   Batch loss: 0.026247   \n",
      "Epoch: 10/40    Batch: 1836   Batch loss: 0.010562   \n",
      "Epoch: 10/40    Batch: 1837   Batch loss: 0.009322   \n",
      "Epoch: 10/40    Batch: 1838   Batch loss: 0.066930   \n",
      "Epoch: 10/40    Batch: 1839   Batch loss: 0.037028   \n",
      "Epoch: 10/40    Batch: 1840   Batch loss: 0.013900   \n",
      "Epoch: 10/40    Batch: 1841   Batch loss: 0.004583   \n",
      "Epoch: 10/40    Batch: 1842   Batch loss: 0.046555   \n",
      "Epoch: 10/40    Batch: 1843   Batch loss: 0.011242   \n",
      "Epoch: 10/40    Batch: 1844   Batch loss: 0.105798   \n",
      "Epoch: 10/40    Batch: 1845   Batch loss: 0.088780   \n",
      "Epoch: 10/40    Batch: 1846   Batch loss: 0.048063   \n",
      "Epoch: 10/40    Batch: 1847   Batch loss: 0.105272   \n",
      "Epoch: 10/40    Batch: 1848   Batch loss: 0.010984   \n",
      "Epoch: 10/40    Batch: 1849   Batch loss: 0.013043   \n",
      "Epoch: 10/40    Batch: 1850   Batch loss: 0.043152   \n",
      "Epoch: 10/40    Batch: 1851   Batch loss: 0.198691   \n",
      "Epoch: 10/40    Batch: 1852   Batch loss: 0.006432   \n",
      "Epoch: 10/40    Batch: 1853   Batch loss: 0.141717   \n",
      "Epoch: 10/40    Batch: 1854   Batch loss: 0.078149   \n",
      "Epoch: 10/40    Batch: 1855   Batch loss: 0.046457   \n",
      "Epoch: 10/40    Batch: 1856   Batch loss: 0.045958   \n",
      "Epoch: 10/40    Batch: 1857   Batch loss: 0.100704   \n",
      "Epoch: 10/40    Batch: 1858   Batch loss: 0.017185   \n",
      "Epoch: 10/40    Batch: 1859   Batch loss: 0.017455   \n",
      "Epoch: 10/40    Batch: 1860   Batch loss: 0.191458   \n",
      "Epoch: 10/40    Batch: 1861   Batch loss: 0.014887   \n",
      "Epoch: 10/40    Batch: 1862   Batch loss: 0.097010   \n",
      "Epoch: 10/40    Batch: 1863   Batch loss: 0.141172   \n",
      "Epoch: 10/40    Batch: 1864   Batch loss: 0.018849   \n",
      "Epoch: 10/40    Batch: 1865   Batch loss: 0.018307   \n",
      "Epoch: 10/40    Batch: 1866   Batch loss: 0.018830   \n",
      "Epoch: 10/40    Batch: 1867   Batch loss: 0.164362   \n",
      "Epoch: 10/40    Batch: 1868   Batch loss: 0.036414   \n",
      "Epoch: 10/40    Batch: 1869   Batch loss: 0.019571   \n",
      "Epoch: 10/40    Batch: 1870   Batch loss: 0.063732   \n",
      "Epoch: 10/40    Batch: 1871   Batch loss: 0.120162   \n",
      "Epoch: 10/40    Batch: 1872   Batch loss: 0.019559   \n",
      "Epoch: 10/40    Batch: 1873   Batch loss: 0.014022   \n",
      "Epoch: 10/40    Batch: 1874   Batch loss: 0.119781   \n",
      "Epoch: 10/40    Batch: 1875   Batch loss: 0.111034   \n",
      "Epoch: 10/40    Batch: 1876   Batch loss: 0.018471   \n",
      "Epoch: 10/40    Batch: 1877   Batch loss: 0.033281   \n",
      "Epoch: 10/40    Batch: 1878   Batch loss: 0.026537   \n",
      "Epoch: 10/40    Batch: 1879   Batch loss: 0.170103   \n",
      "Epoch: 10/40    Batch: 1880   Batch loss: 0.094292   \n",
      "Epoch: 10/40    Batch: 1881   Batch loss: 0.043102   \n",
      "Epoch: 10/40    Batch: 1882   Batch loss: 0.034474   \n",
      "Epoch: 10/40    Batch: 1883   Batch loss: 0.013432   \n",
      "Epoch: 10/40    Batch: 1884   Batch loss: 0.038346   \n",
      "Epoch: 10/40    Batch: 1885   Batch loss: 0.017419   \n",
      "Epoch: 10/40    Batch: 1886   Batch loss: 0.069096   \n",
      "Epoch: 10/40    Batch: 1887   Batch loss: 0.088611   \n",
      "Epoch: 10/40    Batch: 1888   Batch loss: 0.027659   \n",
      "Epoch: 10/40    Batch: 1889   Batch loss: 0.041150   \n",
      "Epoch: 10/40    Batch: 1890   Batch loss: 0.067061   \n",
      "Epoch: 10/40    Batch: 1891   Batch loss: 0.186183   \n",
      "Epoch: 10/40    Batch: 1892   Batch loss: 0.041798   \n",
      "Epoch: 10/40    Batch: 1893   Batch loss: 0.157335   \n",
      "Epoch: 10/40    Batch: 1894   Batch loss: 0.046755   \n",
      "Epoch: 10/40    Batch: 1895   Batch loss: 0.117660   \n",
      "Epoch: 10/40    Batch: 1896   Batch loss: 0.028260   \n",
      "Epoch: 10/40    Batch: 1897   Batch loss: 0.009916   \n",
      "Epoch: 10/40    Batch: 1898   Batch loss: 0.050749   \n",
      "Epoch: 10/40    Batch: 1899   Batch loss: 0.035009   \n",
      "Epoch: 10/40    Batch: 1900   Batch loss: 0.137082   \n",
      "Epoch: 10/40    Batch: 1901   Batch loss: 0.022029   \n",
      "Epoch: 10/40    Batch: 1902   Batch loss: 0.061216   \n",
      "Epoch: 10/40    Batch: 1903   Batch loss: 0.141490   \n",
      "Epoch: 10/40    Batch: 1904   Batch loss: 0.083375   \n",
      "Epoch: 10/40    Batch: 1905   Batch loss: 0.193301   \n",
      "Epoch: 10/40    Batch: 1906   Batch loss: 0.076252   \n",
      "Epoch: 10/40    Batch: 1907   Batch loss: 0.089046   \n",
      "Epoch: 10/40    Batch: 1908   Batch loss: 0.122836   \n",
      "Epoch: 10/40    Batch: 1909   Batch loss: 0.026394   \n",
      "Epoch: 10/40    Batch: 1910   Batch loss: 0.030240   \n",
      "Epoch: 10/40    Batch: 1911   Batch loss: 0.068152   \n",
      "Epoch: 10/40    Batch: 1912   Batch loss: 0.074065   \n",
      "Epoch: 10/40    Batch: 1913   Batch loss: 0.046239   \n",
      "Epoch: 10/40    Batch: 1914   Batch loss: 0.020423   \n",
      "Epoch: 10/40    Batch: 1915   Batch loss: 0.094072   \n",
      "Epoch: 10/40    Batch: 1916   Batch loss: 0.055274   \n",
      "Epoch: 10/40    Batch: 1917   Batch loss: 0.014399   \n",
      "Epoch: 10/40    Batch: 1918   Batch loss: 0.102254   \n",
      "Epoch: 10/40    Batch: 1919   Batch loss: 0.012493   \n",
      "Epoch: 10/40    Batch: 1920   Batch loss: 0.100070   \n",
      "Epoch: 10/40    Batch: 1921   Batch loss: 0.037598   \n",
      "Epoch: 10/40    Batch: 1922   Batch loss: 0.030514   \n",
      "Epoch: 10/40    Batch: 1923   Batch loss: 0.191549   \n",
      "Epoch: 10/40    Batch: 1924   Batch loss: 0.269581   \n",
      "Epoch: 10/40    Batch: 1925   Batch loss: 0.013478   \n",
      "Epoch: 10/40    Batch: 1926   Batch loss: 0.085464   \n",
      "Epoch: 10/40    Batch: 1927   Batch loss: 0.081935   \n",
      "Epoch: 10/40    Batch: 1928   Batch loss: 0.025765   \n",
      "Epoch: 10/40    Batch: 1929   Batch loss: 0.039753   \n",
      "Epoch: 10/40    Batch: 1930   Batch loss: 0.077640   \n",
      "Epoch: 10/40    Batch: 1931   Batch loss: 0.122651   \n",
      "Epoch: 10/40    Batch: 1932   Batch loss: 0.087416   \n",
      "Epoch: 10/40    Batch: 1933   Batch loss: 0.029574   \n",
      "Epoch: 10/40    Batch: 1934   Batch loss: 0.022097   \n",
      "Epoch: 10/40    Batch: 1935   Batch loss: 0.018414   \n",
      "Epoch: 10/40    Batch: 1936   Batch loss: 0.057995   \n",
      "Epoch: 10/40    Batch: 1937   Batch loss: 0.077682   \n",
      "Epoch: 10/40    Batch: 1938   Batch loss: 0.073053   \n",
      "Epoch: 10/40    Batch: 1939   Batch loss: 0.019177   \n",
      "Epoch: 10/40    Batch: 1940   Batch loss: 0.265913   \n",
      "Epoch: 10/40    Batch: 1941   Batch loss: 0.061231   \n",
      "Epoch: 10/40    Batch: 1942   Batch loss: 0.034790   \n",
      "Epoch: 10/40    Batch: 1943   Batch loss: 0.025176   \n",
      "Epoch: 10/40    Batch: 1944   Batch loss: 0.023486   \n",
      "Epoch: 10/40    Batch: 1945   Batch loss: 0.118142   \n",
      "Epoch: 10/40    Batch: 1946   Batch loss: 0.048832   \n",
      "Epoch: 10/40    Batch: 1947   Batch loss: 0.017890   \n",
      "Epoch: 10/40    Batch: 1948   Batch loss: 0.036053   \n",
      "Epoch: 10/40    Batch: 1949   Batch loss: 0.085514   \n",
      "Epoch: 10/40    Batch: 1950   Batch loss: 0.058494   \n",
      "Epoch: 10/40    Batch: 1951   Batch loss: 0.008463   \n",
      "Epoch: 10/40    Batch: 1952   Batch loss: 0.066580   \n",
      "Epoch: 10/40    Batch: 1953   Batch loss: 0.082671   \n",
      "Epoch: 10/40    Batch: 1954   Batch loss: 0.041060   \n",
      "Epoch: 10/40    Batch: 1955   Batch loss: 0.047559   \n",
      "Epoch: 10/40    Batch: 1956   Batch loss: 0.081309   \n",
      "Epoch: 10/40    Batch: 1957   Batch loss: 0.006493   \n",
      "Epoch: 10/40    Batch: 1958   Batch loss: 0.167740   \n",
      "Epoch: 10/40    Batch: 1959   Batch loss: 0.097363   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/40    Batch: 1960   Batch loss: 0.078888   \n",
      "Epoch: 10/40    Batch: 1961   Batch loss: 0.055609   \n",
      "Epoch: 10/40    Batch: 1962   Batch loss: 0.091366   \n",
      "Epoch: 10/40    Batch: 1963   Batch loss: 0.042675   \n",
      "Epoch: 10/40    Batch: 1964   Batch loss: 0.074810   \n",
      "Epoch: 10/40    Batch: 1965   Batch loss: 0.073673   \n",
      "Epoch: 10/40    Batch: 1966   Batch loss: 0.040343   \n",
      "Epoch: 10/40    Batch: 1967   Batch loss: 0.053813   \n",
      "Epoch: 10/40    Batch: 1968   Batch loss: 0.034760   \n",
      "Epoch: 10/40    Batch: 1969   Batch loss: 0.126720   \n",
      "Epoch: 10/40    Batch: 1970   Batch loss: 0.053795   \n",
      "Epoch: 10/40    Batch: 1971   Batch loss: 0.185102   \n",
      "Epoch: 10/40    Batch: 1972   Batch loss: 0.011189   \n",
      "Epoch: 10/40    Batch: 1973   Batch loss: 0.032461   \n",
      "Epoch: 10/40    Batch: 1974   Batch loss: 0.039720   \n",
      "Epoch: 10/40    Batch: 1975   Batch loss: 0.063957   \n",
      "Epoch: 10/40    Batch: 1976   Batch loss: 0.063787   \n",
      "Epoch: 10/40    Batch: 1977   Batch loss: 0.129264   \n",
      "Epoch: 10/40    Batch: 1978   Batch loss: 0.136638   \n",
      "Epoch: 10/40    Batch: 1979   Batch loss: 0.007134   \n",
      "Epoch: 10/40    Batch: 1980   Batch loss: 0.204370   \n",
      "Epoch: 10/40    Batch: 1981   Batch loss: 0.117852   \n",
      "Epoch: 10/40    Batch: 1982   Batch loss: 0.034764   \n",
      "Epoch: 10/40    Batch: 1983   Batch loss: 0.104939   \n",
      "Epoch: 10/40    Batch: 1984   Batch loss: 0.063466   \n",
      "Epoch: 10/40    Batch: 1985   Batch loss: 0.085109   \n",
      "Epoch: 10/40    Batch: 1986   Batch loss: 0.120405   \n",
      "Epoch: 10/40    Batch: 1987   Batch loss: 0.173071   \n",
      "Epoch: 10/40    Batch: 1988   Batch loss: 0.019347   \n",
      "Epoch: 10/40    Batch: 1989   Batch loss: 0.040234   \n",
      "Epoch: 10/40    Batch: 1990   Batch loss: 0.031133   \n",
      "Epoch: 10/40    Batch: 1991   Batch loss: 0.053981   \n",
      "Epoch: 10/40    Batch: 1992   Batch loss: 0.016826   \n",
      "Epoch: 10/40    Batch: 1993   Batch loss: 0.047032   \n",
      "Epoch: 10/40    Batch: 1994   Batch loss: 0.059506   \n",
      "Epoch: 10/40    Batch: 1995   Batch loss: 0.076571   \n",
      "Epoch: 10/40    Batch: 1996   Batch loss: 0.114850   \n",
      "Epoch: 10/40    Batch: 1997   Batch loss: 0.035810   \n",
      "Epoch: 10/40    Batch: 1998   Batch loss: 0.013571   \n",
      "Epoch: 10/40    Batch: 1999   Batch loss: 0.042631   \n",
      "Epoch: 10/40    Batch: 2000   Batch loss: 0.088022   \n",
      "Epoch: 20/40    Batch: 3801   Batch loss: 0.035597   \n",
      "Epoch: 20/40    Batch: 3802   Batch loss: 0.062450   \n",
      "Epoch: 20/40    Batch: 3803   Batch loss: 0.014329   \n",
      "Epoch: 20/40    Batch: 3804   Batch loss: 0.015353   \n",
      "Epoch: 20/40    Batch: 3805   Batch loss: 0.002436   \n",
      "Epoch: 20/40    Batch: 3806   Batch loss: 0.042168   \n",
      "Epoch: 20/40    Batch: 3807   Batch loss: 0.066588   \n",
      "Epoch: 20/40    Batch: 3808   Batch loss: 0.050021   \n",
      "Epoch: 20/40    Batch: 3809   Batch loss: 0.023492   \n",
      "Epoch: 20/40    Batch: 3810   Batch loss: 0.044125   \n",
      "Epoch: 20/40    Batch: 3811   Batch loss: 0.011026   \n",
      "Epoch: 20/40    Batch: 3812   Batch loss: 0.021557   \n",
      "Epoch: 20/40    Batch: 3813   Batch loss: 0.015669   \n",
      "Epoch: 20/40    Batch: 3814   Batch loss: 0.049673   \n",
      "Epoch: 20/40    Batch: 3815   Batch loss: 0.044548   \n",
      "Epoch: 20/40    Batch: 3816   Batch loss: 0.034434   \n",
      "Epoch: 20/40    Batch: 3817   Batch loss: 0.104221   \n",
      "Epoch: 20/40    Batch: 3818   Batch loss: 0.021475   \n",
      "Epoch: 20/40    Batch: 3819   Batch loss: 0.010347   \n",
      "Epoch: 20/40    Batch: 3820   Batch loss: 0.045884   \n",
      "Epoch: 20/40    Batch: 3821   Batch loss: 0.077936   \n",
      "Epoch: 20/40    Batch: 3822   Batch loss: 0.056077   \n",
      "Epoch: 20/40    Batch: 3823   Batch loss: 0.046123   \n",
      "Epoch: 20/40    Batch: 3824   Batch loss: 0.008353   \n",
      "Epoch: 20/40    Batch: 3825   Batch loss: 0.004413   \n",
      "Epoch: 20/40    Batch: 3826   Batch loss: 0.012300   \n",
      "Epoch: 20/40    Batch: 3827   Batch loss: 0.084755   \n",
      "Epoch: 20/40    Batch: 3828   Batch loss: 0.078950   \n",
      "Epoch: 20/40    Batch: 3829   Batch loss: 0.008367   \n",
      "Epoch: 20/40    Batch: 3830   Batch loss: 0.024117   \n",
      "Epoch: 20/40    Batch: 3831   Batch loss: 0.014801   \n",
      "Epoch: 20/40    Batch: 3832   Batch loss: 0.009209   \n",
      "Epoch: 20/40    Batch: 3833   Batch loss: 0.003592   \n",
      "Epoch: 20/40    Batch: 3834   Batch loss: 0.048375   \n",
      "Epoch: 20/40    Batch: 3835   Batch loss: 0.055254   \n",
      "Epoch: 20/40    Batch: 3836   Batch loss: 0.109038   \n",
      "Epoch: 20/40    Batch: 3837   Batch loss: 0.005119   \n",
      "Epoch: 20/40    Batch: 3838   Batch loss: 0.023867   \n",
      "Epoch: 20/40    Batch: 3839   Batch loss: 0.038092   \n",
      "Epoch: 20/40    Batch: 3840   Batch loss: 0.063730   \n",
      "Epoch: 20/40    Batch: 3841   Batch loss: 0.078375   \n",
      "Epoch: 20/40    Batch: 3842   Batch loss: 0.019168   \n",
      "Epoch: 20/40    Batch: 3843   Batch loss: 0.073306   \n",
      "Epoch: 20/40    Batch: 3844   Batch loss: 0.016934   \n",
      "Epoch: 20/40    Batch: 3845   Batch loss: 0.017376   \n",
      "Epoch: 20/40    Batch: 3846   Batch loss: 0.026327   \n",
      "Epoch: 20/40    Batch: 3847   Batch loss: 0.053909   \n",
      "Epoch: 20/40    Batch: 3848   Batch loss: 0.045578   \n",
      "Epoch: 20/40    Batch: 3849   Batch loss: 0.008869   \n",
      "Epoch: 20/40    Batch: 3850   Batch loss: 0.074324   \n",
      "Epoch: 20/40    Batch: 3851   Batch loss: 0.018276   \n",
      "Epoch: 20/40    Batch: 3852   Batch loss: 0.041685   \n",
      "Epoch: 20/40    Batch: 3853   Batch loss: 0.035430   \n",
      "Epoch: 20/40    Batch: 3854   Batch loss: 0.084536   \n",
      "Epoch: 20/40    Batch: 3855   Batch loss: 0.060751   \n",
      "Epoch: 20/40    Batch: 3856   Batch loss: 0.032835   \n",
      "Epoch: 20/40    Batch: 3857   Batch loss: 0.044013   \n",
      "Epoch: 20/40    Batch: 3858   Batch loss: 0.025394   \n",
      "Epoch: 20/40    Batch: 3859   Batch loss: 0.040777   \n",
      "Epoch: 20/40    Batch: 3860   Batch loss: 0.013882   \n",
      "Epoch: 20/40    Batch: 3861   Batch loss: 0.110909   \n",
      "Epoch: 20/40    Batch: 3862   Batch loss: 0.013640   \n",
      "Epoch: 20/40    Batch: 3863   Batch loss: 0.024317   \n",
      "Epoch: 20/40    Batch: 3864   Batch loss: 0.178287   \n",
      "Epoch: 20/40    Batch: 3865   Batch loss: 0.025617   \n",
      "Epoch: 20/40    Batch: 3866   Batch loss: 0.038165   \n",
      "Epoch: 20/40    Batch: 3867   Batch loss: 0.019549   \n",
      "Epoch: 20/40    Batch: 3868   Batch loss: 0.007970   \n",
      "Epoch: 20/40    Batch: 3869   Batch loss: 0.038988   \n",
      "Epoch: 20/40    Batch: 3870   Batch loss: 0.063969   \n",
      "Epoch: 20/40    Batch: 3871   Batch loss: 0.056379   \n",
      "Epoch: 20/40    Batch: 3872   Batch loss: 0.070522   \n",
      "Epoch: 20/40    Batch: 3873   Batch loss: 0.041145   \n",
      "Epoch: 20/40    Batch: 3874   Batch loss: 0.007568   \n",
      "Epoch: 20/40    Batch: 3875   Batch loss: 0.041258   \n",
      "Epoch: 20/40    Batch: 3876   Batch loss: 0.072069   \n",
      "Epoch: 20/40    Batch: 3877   Batch loss: 0.002397   \n",
      "Epoch: 20/40    Batch: 3878   Batch loss: 0.006803   \n",
      "Epoch: 20/40    Batch: 3879   Batch loss: 0.068071   \n",
      "Epoch: 20/40    Batch: 3880   Batch loss: 0.024280   \n",
      "Epoch: 20/40    Batch: 3881   Batch loss: 0.336780   \n",
      "Epoch: 20/40    Batch: 3882   Batch loss: 0.059022   \n",
      "Epoch: 20/40    Batch: 3883   Batch loss: 0.028086   \n",
      "Epoch: 20/40    Batch: 3884   Batch loss: 0.039051   \n",
      "Epoch: 20/40    Batch: 3885   Batch loss: 0.125497   \n",
      "Epoch: 20/40    Batch: 3886   Batch loss: 0.057015   \n",
      "Epoch: 20/40    Batch: 3887   Batch loss: 0.023504   \n",
      "Epoch: 20/40    Batch: 3888   Batch loss: 0.070131   \n",
      "Epoch: 20/40    Batch: 3889   Batch loss: 0.094753   \n",
      "Epoch: 20/40    Batch: 3890   Batch loss: 0.050415   \n",
      "Epoch: 20/40    Batch: 3891   Batch loss: 0.026135   \n",
      "Epoch: 20/40    Batch: 3892   Batch loss: 0.010770   \n",
      "Epoch: 20/40    Batch: 3893   Batch loss: 0.041112   \n",
      "Epoch: 20/40    Batch: 3894   Batch loss: 0.130417   \n",
      "Epoch: 20/40    Batch: 3895   Batch loss: 0.032025   \n",
      "Epoch: 20/40    Batch: 3896   Batch loss: 0.109973   \n",
      "Epoch: 20/40    Batch: 3897   Batch loss: 0.056037   \n",
      "Epoch: 20/40    Batch: 3898   Batch loss: 0.014848   \n",
      "Epoch: 20/40    Batch: 3899   Batch loss: 0.063779   \n",
      "Epoch: 20/40    Batch: 3900   Batch loss: 0.036334   \n",
      "Epoch: 20/40    Batch: 3901   Batch loss: 0.014210   \n",
      "Epoch: 20/40    Batch: 3902   Batch loss: 0.109814   \n",
      "Epoch: 20/40    Batch: 3903   Batch loss: 0.261270   \n",
      "Epoch: 20/40    Batch: 3904   Batch loss: 0.035658   \n",
      "Epoch: 20/40    Batch: 3905   Batch loss: 0.019459   \n",
      "Epoch: 20/40    Batch: 3906   Batch loss: 0.004923   \n",
      "Epoch: 20/40    Batch: 3907   Batch loss: 0.012410   \n",
      "Epoch: 20/40    Batch: 3908   Batch loss: 0.021468   \n",
      "Epoch: 20/40    Batch: 3909   Batch loss: 0.030316   \n",
      "Epoch: 20/40    Batch: 3910   Batch loss: 0.042167   \n",
      "Epoch: 20/40    Batch: 3911   Batch loss: 0.053604   \n",
      "Epoch: 20/40    Batch: 3912   Batch loss: 0.006761   \n",
      "Epoch: 20/40    Batch: 3913   Batch loss: 0.005617   \n",
      "Epoch: 20/40    Batch: 3914   Batch loss: 0.166093   \n",
      "Epoch: 20/40    Batch: 3915   Batch loss: 0.103289   \n",
      "Epoch: 20/40    Batch: 3916   Batch loss: 0.113649   \n",
      "Epoch: 20/40    Batch: 3917   Batch loss: 0.109611   \n",
      "Epoch: 20/40    Batch: 3918   Batch loss: 0.019003   \n",
      "Epoch: 20/40    Batch: 3919   Batch loss: 0.031331   \n",
      "Epoch: 20/40    Batch: 3920   Batch loss: 0.079720   \n",
      "Epoch: 20/40    Batch: 3921   Batch loss: 0.040920   \n",
      "Epoch: 20/40    Batch: 3922   Batch loss: 0.060956   \n",
      "Epoch: 20/40    Batch: 3923   Batch loss: 0.130287   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20/40    Batch: 3924   Batch loss: 0.003401   \n",
      "Epoch: 20/40    Batch: 3925   Batch loss: 0.017803   \n",
      "Epoch: 20/40    Batch: 3926   Batch loss: 0.053188   \n",
      "Epoch: 20/40    Batch: 3927   Batch loss: 0.010095   \n",
      "Epoch: 20/40    Batch: 3928   Batch loss: 0.052202   \n",
      "Epoch: 20/40    Batch: 3929   Batch loss: 0.045774   \n",
      "Epoch: 20/40    Batch: 3930   Batch loss: 0.212911   \n",
      "Epoch: 20/40    Batch: 3931   Batch loss: 0.083414   \n",
      "Epoch: 20/40    Batch: 3932   Batch loss: 0.010109   \n",
      "Epoch: 20/40    Batch: 3933   Batch loss: 0.024950   \n",
      "Epoch: 20/40    Batch: 3934   Batch loss: 0.038372   \n",
      "Epoch: 20/40    Batch: 3935   Batch loss: 0.027190   \n",
      "Epoch: 20/40    Batch: 3936   Batch loss: 0.044623   \n",
      "Epoch: 20/40    Batch: 3937   Batch loss: 0.039346   \n",
      "Epoch: 20/40    Batch: 3938   Batch loss: 0.007246   \n",
      "Epoch: 20/40    Batch: 3939   Batch loss: 0.010687   \n",
      "Epoch: 20/40    Batch: 3940   Batch loss: 0.071305   \n",
      "Epoch: 20/40    Batch: 3941   Batch loss: 0.027317   \n",
      "Epoch: 20/40    Batch: 3942   Batch loss: 0.044467   \n",
      "Epoch: 20/40    Batch: 3943   Batch loss: 0.014942   \n",
      "Epoch: 20/40    Batch: 3944   Batch loss: 0.002210   \n",
      "Epoch: 20/40    Batch: 3945   Batch loss: 0.061443   \n",
      "Epoch: 20/40    Batch: 3946   Batch loss: 0.024199   \n",
      "Epoch: 20/40    Batch: 3947   Batch loss: 0.035168   \n",
      "Epoch: 20/40    Batch: 3948   Batch loss: 0.082166   \n",
      "Epoch: 20/40    Batch: 3949   Batch loss: 0.043901   \n",
      "Epoch: 20/40    Batch: 3950   Batch loss: 0.041600   \n",
      "Epoch: 20/40    Batch: 3951   Batch loss: 0.039344   \n",
      "Epoch: 20/40    Batch: 3952   Batch loss: 0.028911   \n",
      "Epoch: 20/40    Batch: 3953   Batch loss: 0.002412   \n",
      "Epoch: 20/40    Batch: 3954   Batch loss: 0.010148   \n",
      "Epoch: 20/40    Batch: 3955   Batch loss: 0.009140   \n",
      "Epoch: 20/40    Batch: 3956   Batch loss: 0.132834   \n",
      "Epoch: 20/40    Batch: 3957   Batch loss: 0.002095   \n",
      "Epoch: 20/40    Batch: 3958   Batch loss: 0.141126   \n",
      "Epoch: 20/40    Batch: 3959   Batch loss: 0.010448   \n",
      "Epoch: 20/40    Batch: 3960   Batch loss: 0.111405   \n",
      "Epoch: 20/40    Batch: 3961   Batch loss: 0.015385   \n",
      "Epoch: 20/40    Batch: 3962   Batch loss: 0.042542   \n",
      "Epoch: 20/40    Batch: 3963   Batch loss: 0.031456   \n",
      "Epoch: 20/40    Batch: 3964   Batch loss: 0.010773   \n",
      "Epoch: 20/40    Batch: 3965   Batch loss: 0.016971   \n",
      "Epoch: 20/40    Batch: 3966   Batch loss: 0.090904   \n",
      "Epoch: 20/40    Batch: 3967   Batch loss: 0.022199   \n",
      "Epoch: 20/40    Batch: 3968   Batch loss: 0.039133   \n",
      "Epoch: 20/40    Batch: 3969   Batch loss: 0.020247   \n",
      "Epoch: 20/40    Batch: 3970   Batch loss: 0.041443   \n",
      "Epoch: 20/40    Batch: 3971   Batch loss: 0.003163   \n",
      "Epoch: 20/40    Batch: 3972   Batch loss: 0.006646   \n",
      "Epoch: 20/40    Batch: 3973   Batch loss: 0.017919   \n",
      "Epoch: 20/40    Batch: 3974   Batch loss: 0.022006   \n",
      "Epoch: 20/40    Batch: 3975   Batch loss: 0.017430   \n",
      "Epoch: 20/40    Batch: 3976   Batch loss: 0.020588   \n",
      "Epoch: 20/40    Batch: 3977   Batch loss: 0.089291   \n",
      "Epoch: 20/40    Batch: 3978   Batch loss: 0.162835   \n",
      "Epoch: 20/40    Batch: 3979   Batch loss: 0.045910   \n",
      "Epoch: 20/40    Batch: 3980   Batch loss: 0.087769   \n",
      "Epoch: 20/40    Batch: 3981   Batch loss: 0.026992   \n",
      "Epoch: 20/40    Batch: 3982   Batch loss: 0.082088   \n",
      "Epoch: 20/40    Batch: 3983   Batch loss: 0.198295   \n",
      "Epoch: 20/40    Batch: 3984   Batch loss: 0.111107   \n",
      "Epoch: 20/40    Batch: 3985   Batch loss: 0.008255   \n",
      "Epoch: 20/40    Batch: 3986   Batch loss: 0.016366   \n",
      "Epoch: 20/40    Batch: 3987   Batch loss: 0.013767   \n",
      "Epoch: 20/40    Batch: 3988   Batch loss: 0.255655   \n",
      "Epoch: 20/40    Batch: 3989   Batch loss: 0.022345   \n",
      "Epoch: 20/40    Batch: 3990   Batch loss: 0.021085   \n",
      "Epoch: 20/40    Batch: 3991   Batch loss: 0.018900   \n",
      "Epoch: 20/40    Batch: 3992   Batch loss: 0.111378   \n",
      "Epoch: 20/40    Batch: 3993   Batch loss: 0.039498   \n",
      "Epoch: 20/40    Batch: 3994   Batch loss: 0.071049   \n",
      "Epoch: 20/40    Batch: 3995   Batch loss: 0.036321   \n",
      "Epoch: 20/40    Batch: 3996   Batch loss: 0.102219   \n",
      "Epoch: 20/40    Batch: 3997   Batch loss: 0.042165   \n",
      "Epoch: 20/40    Batch: 3998   Batch loss: 0.045636   \n",
      "Epoch: 20/40    Batch: 3999   Batch loss: 0.033830   \n",
      "Epoch: 20/40    Batch: 4000   Batch loss: 0.008019   \n",
      "Epoch: 30/40    Batch: 5801   Batch loss: 0.015591   \n",
      "Epoch: 30/40    Batch: 5802   Batch loss: 0.046833   \n",
      "Epoch: 30/40    Batch: 5803   Batch loss: 0.028370   \n",
      "Epoch: 30/40    Batch: 5804   Batch loss: 0.084111   \n",
      "Epoch: 30/40    Batch: 5805   Batch loss: 0.009625   \n",
      "Epoch: 30/40    Batch: 5806   Batch loss: 0.006106   \n",
      "Epoch: 30/40    Batch: 5807   Batch loss: 0.014813   \n",
      "Epoch: 30/40    Batch: 5808   Batch loss: 0.030588   \n",
      "Epoch: 30/40    Batch: 5809   Batch loss: 0.061889   \n",
      "Epoch: 30/40    Batch: 5810   Batch loss: 0.005450   \n",
      "Epoch: 30/40    Batch: 5811   Batch loss: 0.007053   \n",
      "Epoch: 30/40    Batch: 5812   Batch loss: 0.010374   \n",
      "Epoch: 30/40    Batch: 5813   Batch loss: 0.018544   \n",
      "Epoch: 30/40    Batch: 5814   Batch loss: 0.010422   \n",
      "Epoch: 30/40    Batch: 5815   Batch loss: 0.006597   \n",
      "Epoch: 30/40    Batch: 5816   Batch loss: 0.029941   \n",
      "Epoch: 30/40    Batch: 5817   Batch loss: 0.032289   \n",
      "Epoch: 30/40    Batch: 5818   Batch loss: 0.021905   \n",
      "Epoch: 30/40    Batch: 5819   Batch loss: 0.000149   \n",
      "Epoch: 30/40    Batch: 5820   Batch loss: 0.054280   \n",
      "Epoch: 30/40    Batch: 5821   Batch loss: 0.009024   \n",
      "Epoch: 30/40    Batch: 5822   Batch loss: 0.018930   \n",
      "Epoch: 30/40    Batch: 5823   Batch loss: 0.007240   \n",
      "Epoch: 30/40    Batch: 5824   Batch loss: 0.022294   \n",
      "Epoch: 30/40    Batch: 5825   Batch loss: 0.082648   \n",
      "Epoch: 30/40    Batch: 5826   Batch loss: 0.010447   \n",
      "Epoch: 30/40    Batch: 5827   Batch loss: 0.025113   \n",
      "Epoch: 30/40    Batch: 5828   Batch loss: 0.022682   \n",
      "Epoch: 30/40    Batch: 5829   Batch loss: 0.073034   \n",
      "Epoch: 30/40    Batch: 5830   Batch loss: 0.017091   \n",
      "Epoch: 30/40    Batch: 5831   Batch loss: 0.002758   \n",
      "Epoch: 30/40    Batch: 5832   Batch loss: 0.007288   \n",
      "Epoch: 30/40    Batch: 5833   Batch loss: 0.019920   \n",
      "Epoch: 30/40    Batch: 5834   Batch loss: 0.046864   \n",
      "Epoch: 30/40    Batch: 5835   Batch loss: 0.029686   \n",
      "Epoch: 30/40    Batch: 5836   Batch loss: 0.017689   \n",
      "Epoch: 30/40    Batch: 5837   Batch loss: 0.016063   \n",
      "Epoch: 30/40    Batch: 5838   Batch loss: 0.014343   \n",
      "Epoch: 30/40    Batch: 5839   Batch loss: 0.004741   \n",
      "Epoch: 30/40    Batch: 5840   Batch loss: 0.002687   \n",
      "Epoch: 30/40    Batch: 5841   Batch loss: 0.004107   \n",
      "Epoch: 30/40    Batch: 5842   Batch loss: 0.000907   \n",
      "Epoch: 30/40    Batch: 5843   Batch loss: 0.001843   \n",
      "Epoch: 30/40    Batch: 5844   Batch loss: 0.041964   \n",
      "Epoch: 30/40    Batch: 5845   Batch loss: 0.057631   \n",
      "Epoch: 30/40    Batch: 5846   Batch loss: 0.077891   \n",
      "Epoch: 30/40    Batch: 5847   Batch loss: 0.004393   \n",
      "Epoch: 30/40    Batch: 5848   Batch loss: 0.009299   \n",
      "Epoch: 30/40    Batch: 5849   Batch loss: 0.034482   \n",
      "Epoch: 30/40    Batch: 5850   Batch loss: 0.020263   \n",
      "Epoch: 30/40    Batch: 5851   Batch loss: 0.004156   \n",
      "Epoch: 30/40    Batch: 5852   Batch loss: 0.029927   \n",
      "Epoch: 30/40    Batch: 5853   Batch loss: 0.091063   \n",
      "Epoch: 30/40    Batch: 5854   Batch loss: 0.009718   \n",
      "Epoch: 30/40    Batch: 5855   Batch loss: 0.002194   \n",
      "Epoch: 30/40    Batch: 5856   Batch loss: 0.011967   \n",
      "Epoch: 30/40    Batch: 5857   Batch loss: 0.010779   \n",
      "Epoch: 30/40    Batch: 5858   Batch loss: 0.005570   \n",
      "Epoch: 30/40    Batch: 5859   Batch loss: 0.006085   \n",
      "Epoch: 30/40    Batch: 5860   Batch loss: 0.025854   \n",
      "Epoch: 30/40    Batch: 5861   Batch loss: 0.050809   \n",
      "Epoch: 30/40    Batch: 5862   Batch loss: 0.004353   \n",
      "Epoch: 30/40    Batch: 5863   Batch loss: 0.091858   \n",
      "Epoch: 30/40    Batch: 5864   Batch loss: 0.071392   \n",
      "Epoch: 30/40    Batch: 5865   Batch loss: 0.191797   \n",
      "Epoch: 30/40    Batch: 5866   Batch loss: 0.021574   \n",
      "Epoch: 30/40    Batch: 5867   Batch loss: 0.057580   \n",
      "Epoch: 30/40    Batch: 5868   Batch loss: 0.060743   \n",
      "Epoch: 30/40    Batch: 5869   Batch loss: 0.040467   \n",
      "Epoch: 30/40    Batch: 5870   Batch loss: 0.000430   \n",
      "Epoch: 30/40    Batch: 5871   Batch loss: 0.075596   \n",
      "Epoch: 30/40    Batch: 5872   Batch loss: 0.003765   \n",
      "Epoch: 30/40    Batch: 5873   Batch loss: 0.052514   \n",
      "Epoch: 30/40    Batch: 5874   Batch loss: 0.019218   \n",
      "Epoch: 30/40    Batch: 5875   Batch loss: 0.050468   \n",
      "Epoch: 30/40    Batch: 5876   Batch loss: 0.063895   \n",
      "Epoch: 30/40    Batch: 5877   Batch loss: 0.160724   \n",
      "Epoch: 30/40    Batch: 5878   Batch loss: 0.037904   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30/40    Batch: 5879   Batch loss: 0.026570   \n",
      "Epoch: 30/40    Batch: 5880   Batch loss: 0.011934   \n",
      "Epoch: 30/40    Batch: 5881   Batch loss: 0.066586   \n",
      "Epoch: 30/40    Batch: 5882   Batch loss: 0.018693   \n",
      "Epoch: 30/40    Batch: 5883   Batch loss: 0.037918   \n",
      "Epoch: 30/40    Batch: 5884   Batch loss: 0.019687   \n",
      "Epoch: 30/40    Batch: 5885   Batch loss: 0.016421   \n",
      "Epoch: 30/40    Batch: 5886   Batch loss: 0.069314   \n",
      "Epoch: 30/40    Batch: 5887   Batch loss: 0.040523   \n",
      "Epoch: 30/40    Batch: 5888   Batch loss: 0.036897   \n",
      "Epoch: 30/40    Batch: 5889   Batch loss: 0.067309   \n",
      "Epoch: 30/40    Batch: 5890   Batch loss: 0.068891   \n",
      "Epoch: 30/40    Batch: 5891   Batch loss: 0.017332   \n",
      "Epoch: 30/40    Batch: 5892   Batch loss: 0.004283   \n",
      "Epoch: 30/40    Batch: 5893   Batch loss: 0.051708   \n",
      "Epoch: 30/40    Batch: 5894   Batch loss: 0.088505   \n",
      "Epoch: 30/40    Batch: 5895   Batch loss: 0.018109   \n",
      "Epoch: 30/40    Batch: 5896   Batch loss: 0.018295   \n",
      "Epoch: 30/40    Batch: 5897   Batch loss: 0.024623   \n",
      "Epoch: 30/40    Batch: 5898   Batch loss: 0.025878   \n",
      "Epoch: 30/40    Batch: 5899   Batch loss: 0.012553   \n",
      "Epoch: 30/40    Batch: 5900   Batch loss: 0.046885   \n",
      "Epoch: 30/40    Batch: 5901   Batch loss: 0.079208   \n",
      "Epoch: 30/40    Batch: 5902   Batch loss: 0.062731   \n",
      "Epoch: 30/40    Batch: 5903   Batch loss: 0.078975   \n",
      "Epoch: 30/40    Batch: 5904   Batch loss: 0.011889   \n",
      "Epoch: 30/40    Batch: 5905   Batch loss: 0.002579   \n",
      "Epoch: 30/40    Batch: 5906   Batch loss: 0.203983   \n",
      "Epoch: 30/40    Batch: 5907   Batch loss: 0.070921   \n",
      "Epoch: 30/40    Batch: 5908   Batch loss: 0.016936   \n",
      "Epoch: 30/40    Batch: 5909   Batch loss: 0.005771   \n",
      "Epoch: 30/40    Batch: 5910   Batch loss: 0.049539   \n",
      "Epoch: 30/40    Batch: 5911   Batch loss: 0.011016   \n",
      "Epoch: 30/40    Batch: 5912   Batch loss: 0.016753   \n",
      "Epoch: 30/40    Batch: 5913   Batch loss: 0.009475   \n",
      "Epoch: 30/40    Batch: 5914   Batch loss: 0.184155   \n",
      "Epoch: 30/40    Batch: 5915   Batch loss: 0.080766   \n",
      "Epoch: 30/40    Batch: 5916   Batch loss: 0.006795   \n",
      "Epoch: 30/40    Batch: 5917   Batch loss: 0.116565   \n",
      "Epoch: 30/40    Batch: 5918   Batch loss: 0.002431   \n",
      "Epoch: 30/40    Batch: 5919   Batch loss: 0.004644   \n",
      "Epoch: 30/40    Batch: 5920   Batch loss: 0.019983   \n",
      "Epoch: 30/40    Batch: 5921   Batch loss: 0.068544   \n",
      "Epoch: 30/40    Batch: 5922   Batch loss: 0.015631   \n",
      "Epoch: 30/40    Batch: 5923   Batch loss: 0.040354   \n",
      "Epoch: 30/40    Batch: 5924   Batch loss: 0.062383   \n",
      "Epoch: 30/40    Batch: 5925   Batch loss: 0.045913   \n",
      "Epoch: 30/40    Batch: 5926   Batch loss: 0.023719   \n",
      "Epoch: 30/40    Batch: 5927   Batch loss: 0.015155   \n",
      "Epoch: 30/40    Batch: 5928   Batch loss: 0.036862   \n",
      "Epoch: 30/40    Batch: 5929   Batch loss: 0.007877   \n",
      "Epoch: 30/40    Batch: 5930   Batch loss: 0.010648   \n",
      "Epoch: 30/40    Batch: 5931   Batch loss: 0.002546   \n",
      "Epoch: 30/40    Batch: 5932   Batch loss: 0.045661   \n",
      "Epoch: 30/40    Batch: 5933   Batch loss: 0.021833   \n",
      "Epoch: 30/40    Batch: 5934   Batch loss: 0.026431   \n",
      "Epoch: 30/40    Batch: 5935   Batch loss: 0.117628   \n",
      "Epoch: 30/40    Batch: 5936   Batch loss: 0.084318   \n",
      "Epoch: 30/40    Batch: 5937   Batch loss: 0.021628   \n",
      "Epoch: 30/40    Batch: 5938   Batch loss: 0.011019   \n",
      "Epoch: 30/40    Batch: 5939   Batch loss: 0.025617   \n",
      "Epoch: 30/40    Batch: 5940   Batch loss: 0.025176   \n",
      "Epoch: 30/40    Batch: 5941   Batch loss: 0.028060   \n",
      "Epoch: 30/40    Batch: 5942   Batch loss: 0.008999   \n",
      "Epoch: 30/40    Batch: 5943   Batch loss: 0.008347   \n",
      "Epoch: 30/40    Batch: 5944   Batch loss: 0.055368   \n",
      "Epoch: 30/40    Batch: 5945   Batch loss: 0.048504   \n",
      "Epoch: 30/40    Batch: 5946   Batch loss: 0.046267   \n",
      "Epoch: 30/40    Batch: 5947   Batch loss: 0.094935   \n",
      "Epoch: 30/40    Batch: 5948   Batch loss: 0.012276   \n",
      "Epoch: 30/40    Batch: 5949   Batch loss: 0.006856   \n",
      "Epoch: 30/40    Batch: 5950   Batch loss: 0.160896   \n",
      "Epoch: 30/40    Batch: 5951   Batch loss: 0.020424   \n",
      "Epoch: 30/40    Batch: 5952   Batch loss: 0.030518   \n",
      "Epoch: 30/40    Batch: 5953   Batch loss: 0.069933   \n",
      "Epoch: 30/40    Batch: 5954   Batch loss: 0.020497   \n",
      "Epoch: 30/40    Batch: 5955   Batch loss: 0.083501   \n",
      "Epoch: 30/40    Batch: 5956   Batch loss: 0.055471   \n",
      "Epoch: 30/40    Batch: 5957   Batch loss: 0.018638   \n",
      "Epoch: 30/40    Batch: 5958   Batch loss: 0.023804   \n",
      "Epoch: 30/40    Batch: 5959   Batch loss: 0.019192   \n",
      "Epoch: 30/40    Batch: 5960   Batch loss: 0.010722   \n",
      "Epoch: 30/40    Batch: 5961   Batch loss: 0.019238   \n",
      "Epoch: 30/40    Batch: 5962   Batch loss: 0.028116   \n",
      "Epoch: 30/40    Batch: 5963   Batch loss: 0.008515   \n",
      "Epoch: 30/40    Batch: 5964   Batch loss: 0.030430   \n",
      "Epoch: 30/40    Batch: 5965   Batch loss: 0.002639   \n",
      "Epoch: 30/40    Batch: 5966   Batch loss: 0.016758   \n",
      "Epoch: 30/40    Batch: 5967   Batch loss: 0.004942   \n",
      "Epoch: 30/40    Batch: 5968   Batch loss: 0.127702   \n",
      "Epoch: 30/40    Batch: 5969   Batch loss: 0.089888   \n",
      "Epoch: 30/40    Batch: 5970   Batch loss: 0.007049   \n",
      "Epoch: 30/40    Batch: 5971   Batch loss: 0.032032   \n",
      "Epoch: 30/40    Batch: 5972   Batch loss: 0.001248   \n",
      "Epoch: 30/40    Batch: 5973   Batch loss: 0.047114   \n",
      "Epoch: 30/40    Batch: 5974   Batch loss: 0.013994   \n",
      "Epoch: 30/40    Batch: 5975   Batch loss: 0.041191   \n",
      "Epoch: 30/40    Batch: 5976   Batch loss: 0.066301   \n",
      "Epoch: 30/40    Batch: 5977   Batch loss: 0.005895   \n",
      "Epoch: 30/40    Batch: 5978   Batch loss: 0.041930   \n",
      "Epoch: 30/40    Batch: 5979   Batch loss: 0.043881   \n",
      "Epoch: 30/40    Batch: 5980   Batch loss: 0.025104   \n",
      "Epoch: 30/40    Batch: 5981   Batch loss: 0.080922   \n",
      "Epoch: 30/40    Batch: 5982   Batch loss: 0.014672   \n",
      "Epoch: 30/40    Batch: 5983   Batch loss: 0.010914   \n",
      "Epoch: 30/40    Batch: 5984   Batch loss: 0.020726   \n",
      "Epoch: 30/40    Batch: 5985   Batch loss: 0.007121   \n",
      "Epoch: 30/40    Batch: 5986   Batch loss: 0.009515   \n",
      "Epoch: 30/40    Batch: 5987   Batch loss: 0.015051   \n",
      "Epoch: 30/40    Batch: 5988   Batch loss: 0.005943   \n",
      "Epoch: 30/40    Batch: 5989   Batch loss: 0.007519   \n",
      "Epoch: 30/40    Batch: 5990   Batch loss: 0.039858   \n",
      "Epoch: 30/40    Batch: 5991   Batch loss: 0.000450   \n",
      "Epoch: 30/40    Batch: 5992   Batch loss: 0.014555   \n",
      "Epoch: 30/40    Batch: 5993   Batch loss: 0.069954   \n",
      "Epoch: 30/40    Batch: 5994   Batch loss: 0.010758   \n",
      "Epoch: 30/40    Batch: 5995   Batch loss: 0.141154   \n",
      "Epoch: 30/40    Batch: 5996   Batch loss: 0.123406   \n",
      "Epoch: 30/40    Batch: 5997   Batch loss: 0.011266   \n",
      "Epoch: 30/40    Batch: 5998   Batch loss: 0.036328   \n",
      "Epoch: 30/40    Batch: 5999   Batch loss: 0.031996   \n",
      "Epoch: 30/40    Batch: 6000   Batch loss: 0.000699   \n",
      "Epoch: 40/40    Batch: 7801   Batch loss: 0.028204   \n",
      "Epoch: 40/40    Batch: 7802   Batch loss: 0.012079   \n",
      "Epoch: 40/40    Batch: 7803   Batch loss: 0.011616   \n",
      "Epoch: 40/40    Batch: 7804   Batch loss: 0.016544   \n",
      "Epoch: 40/40    Batch: 7805   Batch loss: 0.051124   \n",
      "Epoch: 40/40    Batch: 7806   Batch loss: 0.018264   \n",
      "Epoch: 40/40    Batch: 7807   Batch loss: 0.027352   \n",
      "Epoch: 40/40    Batch: 7808   Batch loss: 0.008985   \n",
      "Epoch: 40/40    Batch: 7809   Batch loss: 0.002793   \n",
      "Epoch: 40/40    Batch: 7810   Batch loss: 0.038055   \n",
      "Epoch: 40/40    Batch: 7811   Batch loss: 0.014547   \n",
      "Epoch: 40/40    Batch: 7812   Batch loss: 0.001289   \n",
      "Epoch: 40/40    Batch: 7813   Batch loss: 0.002442   \n",
      "Epoch: 40/40    Batch: 7814   Batch loss: 0.013415   \n",
      "Epoch: 40/40    Batch: 7815   Batch loss: 0.005319   \n",
      "Epoch: 40/40    Batch: 7816   Batch loss: 0.011507   \n",
      "Epoch: 40/40    Batch: 7817   Batch loss: 0.064027   \n",
      "Epoch: 40/40    Batch: 7818   Batch loss: 0.004090   \n",
      "Epoch: 40/40    Batch: 7819   Batch loss: 0.021868   \n",
      "Epoch: 40/40    Batch: 7820   Batch loss: 0.056403   \n",
      "Epoch: 40/40    Batch: 7821   Batch loss: 0.000397   \n",
      "Epoch: 40/40    Batch: 7822   Batch loss: 0.053796   \n",
      "Epoch: 40/40    Batch: 7823   Batch loss: 0.002866   \n",
      "Epoch: 40/40    Batch: 7824   Batch loss: 0.013706   \n",
      "Epoch: 40/40    Batch: 7825   Batch loss: 0.018666   \n",
      "Epoch: 40/40    Batch: 7826   Batch loss: 0.008154   \n",
      "Epoch: 40/40    Batch: 7827   Batch loss: 0.012769   \n",
      "Epoch: 40/40    Batch: 7828   Batch loss: 0.010128   \n",
      "Epoch: 40/40    Batch: 7829   Batch loss: 0.104194   \n",
      "Epoch: 40/40    Batch: 7830   Batch loss: 0.001360   \n",
      "Epoch: 40/40    Batch: 7831   Batch loss: 0.063013   \n",
      "Epoch: 40/40    Batch: 7832   Batch loss: 0.013398   \n",
      "Epoch: 40/40    Batch: 7833   Batch loss: 0.009593   \n",
      "Epoch: 40/40    Batch: 7834   Batch loss: 0.115895   \n",
      "Epoch: 40/40    Batch: 7835   Batch loss: 0.001871   \n",
      "Epoch: 40/40    Batch: 7836   Batch loss: 0.000714   \n",
      "Epoch: 40/40    Batch: 7837   Batch loss: 0.012171   \n",
      "Epoch: 40/40    Batch: 7838   Batch loss: 0.003673   \n",
      "Epoch: 40/40    Batch: 7839   Batch loss: 0.016002   \n",
      "Epoch: 40/40    Batch: 7840   Batch loss: 0.004171   \n",
      "Epoch: 40/40    Batch: 7841   Batch loss: 0.005925   \n",
      "Epoch: 40/40    Batch: 7842   Batch loss: 0.032210   \n",
      "Epoch: 40/40    Batch: 7843   Batch loss: 0.004775   \n",
      "Epoch: 40/40    Batch: 7844   Batch loss: 0.000141   \n",
      "Epoch: 40/40    Batch: 7845   Batch loss: 0.020344   \n",
      "Epoch: 40/40    Batch: 7846   Batch loss: 0.017535   \n",
      "Epoch: 40/40    Batch: 7847   Batch loss: 0.062015   \n",
      "Epoch: 40/40    Batch: 7848   Batch loss: 0.041400   \n",
      "Epoch: 40/40    Batch: 7849   Batch loss: 0.117499   \n",
      "Epoch: 40/40    Batch: 7850   Batch loss: 0.000835   \n",
      "Epoch: 40/40    Batch: 7851   Batch loss: 0.022787   \n",
      "Epoch: 40/40    Batch: 7852   Batch loss: 0.000369   \n",
      "Epoch: 40/40    Batch: 7853   Batch loss: 0.001830   \n",
      "Epoch: 40/40    Batch: 7854   Batch loss: 0.000388   \n",
      "Epoch: 40/40    Batch: 7855   Batch loss: 0.037950   \n",
      "Epoch: 40/40    Batch: 7856   Batch loss: 0.038683   \n",
      "Epoch: 40/40    Batch: 7857   Batch loss: 0.017022   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 40/40    Batch: 7858   Batch loss: 0.007619   \n",
      "Epoch: 40/40    Batch: 7859   Batch loss: 0.052973   \n",
      "Epoch: 40/40    Batch: 7860   Batch loss: 0.185131   \n",
      "Epoch: 40/40    Batch: 7861   Batch loss: 0.006099   \n",
      "Epoch: 40/40    Batch: 7862   Batch loss: 0.007288   \n",
      "Epoch: 40/40    Batch: 7863   Batch loss: 0.006268   \n",
      "Epoch: 40/40    Batch: 7864   Batch loss: 0.065810   \n",
      "Epoch: 40/40    Batch: 7865   Batch loss: 0.027519   \n",
      "Epoch: 40/40    Batch: 7866   Batch loss: 0.016383   \n",
      "Epoch: 40/40    Batch: 7867   Batch loss: 0.061472   \n",
      "Epoch: 40/40    Batch: 7868   Batch loss: 0.001546   \n",
      "Epoch: 40/40    Batch: 7869   Batch loss: 0.083460   \n",
      "Epoch: 40/40    Batch: 7870   Batch loss: 0.069645   \n",
      "Epoch: 40/40    Batch: 7871   Batch loss: 0.057384   \n",
      "Epoch: 40/40    Batch: 7872   Batch loss: 0.003665   \n",
      "Epoch: 40/40    Batch: 7873   Batch loss: 0.060238   \n",
      "Epoch: 40/40    Batch: 7874   Batch loss: 0.006763   \n",
      "Epoch: 40/40    Batch: 7875   Batch loss: 0.023802   \n",
      "Epoch: 40/40    Batch: 7876   Batch loss: 0.171554   \n",
      "Epoch: 40/40    Batch: 7877   Batch loss: 0.046678   \n",
      "Epoch: 40/40    Batch: 7878   Batch loss: 0.029005   \n",
      "Epoch: 40/40    Batch: 7879   Batch loss: 0.011746   \n",
      "Epoch: 40/40    Batch: 7880   Batch loss: 0.006428   \n",
      "Epoch: 40/40    Batch: 7881   Batch loss: 0.044587   \n",
      "Epoch: 40/40    Batch: 7882   Batch loss: 0.026662   \n",
      "Epoch: 40/40    Batch: 7883   Batch loss: 0.014195   \n",
      "Epoch: 40/40    Batch: 7884   Batch loss: 0.006049   \n",
      "Epoch: 40/40    Batch: 7885   Batch loss: 0.033421   \n",
      "Epoch: 40/40    Batch: 7886   Batch loss: 0.002794   \n",
      "Epoch: 40/40    Batch: 7887   Batch loss: 0.016094   \n",
      "Epoch: 40/40    Batch: 7888   Batch loss: 0.030006   \n",
      "Epoch: 40/40    Batch: 7889   Batch loss: 0.022141   \n",
      "Epoch: 40/40    Batch: 7890   Batch loss: 0.008647   \n",
      "Epoch: 40/40    Batch: 7891   Batch loss: 0.014704   \n",
      "Epoch: 40/40    Batch: 7892   Batch loss: 0.028116   \n",
      "Epoch: 40/40    Batch: 7893   Batch loss: 0.026622   \n",
      "Epoch: 40/40    Batch: 7894   Batch loss: 0.027964   \n",
      "Epoch: 40/40    Batch: 7895   Batch loss: 0.023138   \n",
      "Epoch: 40/40    Batch: 7896   Batch loss: 0.009438   \n",
      "Epoch: 40/40    Batch: 7897   Batch loss: 0.151872   \n",
      "Epoch: 40/40    Batch: 7898   Batch loss: 0.021229   \n",
      "Epoch: 40/40    Batch: 7899   Batch loss: 0.004009   \n",
      "Epoch: 40/40    Batch: 7900   Batch loss: 0.024754   \n",
      "Epoch: 40/40    Batch: 7901   Batch loss: 0.012059   \n",
      "Epoch: 40/40    Batch: 7902   Batch loss: 0.002414   \n",
      "Epoch: 40/40    Batch: 7903   Batch loss: 0.015481   \n",
      "Epoch: 40/40    Batch: 7904   Batch loss: 0.098119   \n",
      "Epoch: 40/40    Batch: 7905   Batch loss: 0.023051   \n",
      "Epoch: 40/40    Batch: 7906   Batch loss: 0.029713   \n",
      "Epoch: 40/40    Batch: 7907   Batch loss: 0.016147   \n",
      "Epoch: 40/40    Batch: 7908   Batch loss: 0.003968   \n",
      "Epoch: 40/40    Batch: 7909   Batch loss: 0.119817   \n",
      "Epoch: 40/40    Batch: 7910   Batch loss: 0.009446   \n",
      "Epoch: 40/40    Batch: 7911   Batch loss: 0.210703   \n",
      "Epoch: 40/40    Batch: 7912   Batch loss: 0.031705   \n",
      "Epoch: 40/40    Batch: 7913   Batch loss: 0.014901   \n",
      "Epoch: 40/40    Batch: 7914   Batch loss: 0.007496   \n",
      "Epoch: 40/40    Batch: 7915   Batch loss: 0.066755   \n",
      "Epoch: 40/40    Batch: 7916   Batch loss: 0.008932   \n",
      "Epoch: 40/40    Batch: 7917   Batch loss: 0.006746   \n",
      "Epoch: 40/40    Batch: 7918   Batch loss: 0.009773   \n",
      "Epoch: 40/40    Batch: 7919   Batch loss: 0.028000   \n",
      "Epoch: 40/40    Batch: 7920   Batch loss: 0.030470   \n",
      "Epoch: 40/40    Batch: 7921   Batch loss: 0.016480   \n",
      "Epoch: 40/40    Batch: 7922   Batch loss: 0.003169   \n",
      "Epoch: 40/40    Batch: 7923   Batch loss: 0.003034   \n",
      "Epoch: 40/40    Batch: 7924   Batch loss: 0.089697   \n",
      "Epoch: 40/40    Batch: 7925   Batch loss: 0.047255   \n",
      "Epoch: 40/40    Batch: 7926   Batch loss: 0.040096   \n",
      "Epoch: 40/40    Batch: 7927   Batch loss: 0.042933   \n",
      "Epoch: 40/40    Batch: 7928   Batch loss: 0.009726   \n",
      "Epoch: 40/40    Batch: 7929   Batch loss: 0.066590   \n",
      "Epoch: 40/40    Batch: 7930   Batch loss: 0.007558   \n",
      "Epoch: 40/40    Batch: 7931   Batch loss: 0.013645   \n",
      "Epoch: 40/40    Batch: 7932   Batch loss: 0.029225   \n",
      "Epoch: 40/40    Batch: 7933   Batch loss: 0.120157   \n",
      "Epoch: 40/40    Batch: 7934   Batch loss: 0.008434   \n",
      "Epoch: 40/40    Batch: 7935   Batch loss: 0.035004   \n",
      "Epoch: 40/40    Batch: 7936   Batch loss: 0.055627   \n",
      "Epoch: 40/40    Batch: 7937   Batch loss: 0.010323   \n",
      "Epoch: 40/40    Batch: 7938   Batch loss: 0.119199   \n",
      "Epoch: 40/40    Batch: 7939   Batch loss: 0.010945   \n",
      "Epoch: 40/40    Batch: 7940   Batch loss: 0.038591   \n",
      "Epoch: 40/40    Batch: 7941   Batch loss: 0.005688   \n",
      "Epoch: 40/40    Batch: 7942   Batch loss: 0.012526   \n",
      "Epoch: 40/40    Batch: 7943   Batch loss: 0.047551   \n",
      "Epoch: 40/40    Batch: 7944   Batch loss: 0.063649   \n",
      "Epoch: 40/40    Batch: 7945   Batch loss: 0.012028   \n",
      "Epoch: 40/40    Batch: 7946   Batch loss: 0.022932   \n",
      "Epoch: 40/40    Batch: 7947   Batch loss: 0.028344   \n",
      "Epoch: 40/40    Batch: 7948   Batch loss: 0.003496   \n",
      "Epoch: 40/40    Batch: 7949   Batch loss: 0.032735   \n",
      "Epoch: 40/40    Batch: 7950   Batch loss: 0.095513   \n",
      "Epoch: 40/40    Batch: 7951   Batch loss: 0.005708   \n",
      "Epoch: 40/40    Batch: 7952   Batch loss: 0.034287   \n",
      "Epoch: 40/40    Batch: 7953   Batch loss: 0.071701   \n",
      "Epoch: 40/40    Batch: 7954   Batch loss: 0.080520   \n",
      "Epoch: 40/40    Batch: 7955   Batch loss: 0.056635   \n",
      "Epoch: 40/40    Batch: 7956   Batch loss: 0.041197   \n",
      "Epoch: 40/40    Batch: 7957   Batch loss: 0.006217   \n",
      "Epoch: 40/40    Batch: 7958   Batch loss: 0.003112   \n",
      "Epoch: 40/40    Batch: 7959   Batch loss: 0.005291   \n",
      "Epoch: 40/40    Batch: 7960   Batch loss: 0.008769   \n",
      "Epoch: 40/40    Batch: 7961   Batch loss: 0.014404   \n",
      "Epoch: 40/40    Batch: 7962   Batch loss: 0.010107   \n",
      "Epoch: 40/40    Batch: 7963   Batch loss: 0.009251   \n",
      "Epoch: 40/40    Batch: 7964   Batch loss: 0.005800   \n",
      "Epoch: 40/40    Batch: 7965   Batch loss: 0.050369   \n",
      "Epoch: 40/40    Batch: 7966   Batch loss: 0.008552   \n",
      "Epoch: 40/40    Batch: 7967   Batch loss: 0.054591   \n",
      "Epoch: 40/40    Batch: 7968   Batch loss: 0.021836   \n",
      "Epoch: 40/40    Batch: 7969   Batch loss: 0.029548   \n",
      "Epoch: 40/40    Batch: 7970   Batch loss: 0.006290   \n",
      "Epoch: 40/40    Batch: 7971   Batch loss: 0.018364   \n",
      "Epoch: 40/40    Batch: 7972   Batch loss: 0.014339   \n",
      "Epoch: 40/40    Batch: 7973   Batch loss: 0.069300   \n",
      "Epoch: 40/40    Batch: 7974   Batch loss: 0.027328   \n",
      "Epoch: 40/40    Batch: 7975   Batch loss: 0.004778   \n",
      "Epoch: 40/40    Batch: 7976   Batch loss: 0.057253   \n",
      "Epoch: 40/40    Batch: 7977   Batch loss: 0.088274   \n",
      "Epoch: 40/40    Batch: 7978   Batch loss: 0.041162   \n",
      "Epoch: 40/40    Batch: 7979   Batch loss: 0.043367   \n",
      "Epoch: 40/40    Batch: 7980   Batch loss: 0.007757   \n",
      "Epoch: 40/40    Batch: 7981   Batch loss: 0.033892   \n",
      "Epoch: 40/40    Batch: 7982   Batch loss: 0.039191   \n",
      "Epoch: 40/40    Batch: 7983   Batch loss: 0.038156   \n",
      "Epoch: 40/40    Batch: 7984   Batch loss: 0.063293   \n",
      "Epoch: 40/40    Batch: 7985   Batch loss: 0.022202   \n",
      "Epoch: 40/40    Batch: 7986   Batch loss: 0.006470   \n",
      "Epoch: 40/40    Batch: 7987   Batch loss: 0.010266   \n",
      "Epoch: 40/40    Batch: 7988   Batch loss: 0.128412   \n",
      "Epoch: 40/40    Batch: 7989   Batch loss: 0.128917   \n",
      "Epoch: 40/40    Batch: 7990   Batch loss: 0.043527   \n",
      "Epoch: 40/40    Batch: 7991   Batch loss: 0.078002   \n",
      "Epoch: 40/40    Batch: 7992   Batch loss: 0.009029   \n",
      "Epoch: 40/40    Batch: 7993   Batch loss: 0.008028   \n",
      "Epoch: 40/40    Batch: 7994   Batch loss: 0.026751   \n",
      "Epoch: 40/40    Batch: 7995   Batch loss: 0.010793   \n",
      "Epoch: 40/40    Batch: 7996   Batch loss: 0.019099   \n",
      "Epoch: 40/40    Batch: 7997   Batch loss: 0.023143   \n",
      "Epoch: 40/40    Batch: 7998   Batch loss: 0.004834   \n",
      "Epoch: 40/40    Batch: 7999   Batch loss: 0.023790   \n",
      "Epoch: 40/40    Batch: 8000   Batch loss: 0.133709   \n",
      "MRR: 0.031436063710520476\n"
     ]
    }
   ],
   "source": [
    "pipeline.evaluate(\n",
    "    model='nb', \n",
    "    pca=0,\n",
    "    pairwise_model='ranknet',\n",
    "    pairwise_top_k=100,\n",
    "    store_model_path = 'models/ranknet.pth'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>model</th>\n",
       "      <th>hyperparameters</th>\n",
       "      <th>pairwise_model</th>\n",
       "      <th>pairwise_k</th>\n",
       "      <th>features</th>\n",
       "      <th>sampling_training</th>\n",
       "      <th>sampling_test</th>\n",
       "      <th>pca</th>\n",
       "      <th>MRR</th>\n",
       "      <th>MAP</th>\n",
       "      <th>nDCG</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>accuracy@50</th>\n",
       "      <th>precision@50</th>\n",
       "      <th>recall@50</th>\n",
       "      <th>f1@50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>LogisticRegression()</td>\n",
       "      <td>{\"C\": 1.0, \"class_weight\": null, \"dual\": false...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[\"w2v_cosine\", \"w2v_euclidean\", \"w2v_manhattan...</td>\n",
       "      <td>9977</td>\n",
       "      <td>451680</td>\n",
       "      <td>0</td>\n",
       "      <td>0.038060</td>\n",
       "      <td>0.865090</td>\n",
       "      <td>0.859446</td>\n",
       "      <td>0.997135</td>\n",
       "      <td>0.993724</td>\n",
       "      <td>0.268969</td>\n",
       "      <td>0.423351</td>\n",
       "      <td>0.690667</td>\n",
       "      <td>0.993289</td>\n",
       "      <td>0.490608</td>\n",
       "      <td>0.656805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>None</td>\n",
       "      <td>LogisticRegression()</td>\n",
       "      <td>{\"C\": 1.0, \"class_weight\": null, \"dual\": false...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[\"pca_comp_0\", \"pca_comp_1\", \"pca_comp_2\", \"pc...</td>\n",
       "      <td>9977</td>\n",
       "      <td>451680</td>\n",
       "      <td>5</td>\n",
       "      <td>0.037385</td>\n",
       "      <td>0.884383</td>\n",
       "      <td>0.875099</td>\n",
       "      <td>0.996925</td>\n",
       "      <td>0.994751</td>\n",
       "      <td>0.214609</td>\n",
       "      <td>0.353051</td>\n",
       "      <td>0.642000</td>\n",
       "      <td>0.994751</td>\n",
       "      <td>0.414661</td>\n",
       "      <td>0.585328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>None</td>\n",
       "      <td>LogisticRegression()</td>\n",
       "      <td>{\"C\": 1.0, \"class_weight\": null, \"dual\": false...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[\"pca_comp_0\", \"pca_comp_1\", \"pca_comp_2\", \"pc...</td>\n",
       "      <td>9977</td>\n",
       "      <td>451680</td>\n",
       "      <td>20</td>\n",
       "      <td>0.039170</td>\n",
       "      <td>0.869707</td>\n",
       "      <td>0.867060</td>\n",
       "      <td>0.997190</td>\n",
       "      <td>0.994036</td>\n",
       "      <td>0.283126</td>\n",
       "      <td>0.440723</td>\n",
       "      <td>0.712000</td>\n",
       "      <td>0.993711</td>\n",
       "      <td>0.524917</td>\n",
       "      <td>0.686957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>None</td>\n",
       "      <td>LogisticRegression()</td>\n",
       "      <td>{\"C\": 1.0, \"class_weight\": null, \"dual\": false...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[\"pca_comp_0\", \"pca_comp_1\", \"pca_comp_2\", \"pc...</td>\n",
       "      <td>9977</td>\n",
       "      <td>451680</td>\n",
       "      <td>25</td>\n",
       "      <td>0.038758</td>\n",
       "      <td>0.870135</td>\n",
       "      <td>0.866985</td>\n",
       "      <td>0.997106</td>\n",
       "      <td>0.997831</td>\n",
       "      <td>0.260476</td>\n",
       "      <td>0.413112</td>\n",
       "      <td>0.685333</td>\n",
       "      <td>0.997701</td>\n",
       "      <td>0.479558</td>\n",
       "      <td>0.647761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>None</td>\n",
       "      <td>GaussianNB()</td>\n",
       "      <td>{\"priors\": null, \"var_smoothing\": 1e-09}</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[\"w2v_cosine\", \"w2v_euclidean\", \"w2v_manhattan...</td>\n",
       "      <td>9977</td>\n",
       "      <td>451680</td>\n",
       "      <td>0</td>\n",
       "      <td>0.036072</td>\n",
       "      <td>0.874882</td>\n",
       "      <td>0.875631</td>\n",
       "      <td>0.997173</td>\n",
       "      <td>0.995943</td>\n",
       "      <td>0.278029</td>\n",
       "      <td>0.434706</td>\n",
       "      <td>0.718000</td>\n",
       "      <td>0.995851</td>\n",
       "      <td>0.532741</td>\n",
       "      <td>0.694143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>None</td>\n",
       "      <td>GaussianNB()</td>\n",
       "      <td>{\"priors\": null, \"var_smoothing\": 1e-09}</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[\"pca_comp_0\", \"pca_comp_1\", \"pca_comp_2\", \"pc...</td>\n",
       "      <td>9977</td>\n",
       "      <td>451680</td>\n",
       "      <td>20</td>\n",
       "      <td>0.021907</td>\n",
       "      <td>0.644065</td>\n",
       "      <td>0.800962</td>\n",
       "      <td>0.997210</td>\n",
       "      <td>0.890432</td>\n",
       "      <td>0.326727</td>\n",
       "      <td>0.478045</td>\n",
       "      <td>0.824000</td>\n",
       "      <td>0.886288</td>\n",
       "      <td>0.730028</td>\n",
       "      <td>0.800604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>None</td>\n",
       "      <td>MLPClassifier()</td>\n",
       "      <td>{\"activation\": \"relu\", \"alpha\": 0.0001, \"batch...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[\"w2v_cosine\", \"w2v_euclidean\", \"w2v_manhattan...</td>\n",
       "      <td>9977</td>\n",
       "      <td>451680</td>\n",
       "      <td>0</td>\n",
       "      <td>0.033422</td>\n",
       "      <td>0.763474</td>\n",
       "      <td>0.840705</td>\n",
       "      <td>0.997140</td>\n",
       "      <td>0.972112</td>\n",
       "      <td>0.276331</td>\n",
       "      <td>0.430335</td>\n",
       "      <td>0.742000</td>\n",
       "      <td>0.971014</td>\n",
       "      <td>0.557007</td>\n",
       "      <td>0.707925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>None</td>\n",
       "      <td>DecisionTreeClassifier()</td>\n",
       "      <td>{\"ccp_alpha\": 0.0, \"class_weight\": null, \"crit...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[\"w2v_cosine\", \"w2v_euclidean\", \"w2v_manhattan...</td>\n",
       "      <td>9977</td>\n",
       "      <td>451680</td>\n",
       "      <td>0</td>\n",
       "      <td>0.008408</td>\n",
       "      <td>0.287001</td>\n",
       "      <td>0.547333</td>\n",
       "      <td>0.996444</td>\n",
       "      <td>0.576775</td>\n",
       "      <td>0.340317</td>\n",
       "      <td>0.428063</td>\n",
       "      <td>0.729333</td>\n",
       "      <td>0.558342</td>\n",
       "      <td>0.998051</td>\n",
       "      <td>0.716084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>None</td>\n",
       "      <td>RandomForestClassifier()</td>\n",
       "      <td>{\"bootstrap\": true, \"ccp_alpha\": 0.0, \"class_w...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[\"w2v_cosine\", \"w2v_euclidean\", \"w2v_manhattan...</td>\n",
       "      <td>9977</td>\n",
       "      <td>451680</td>\n",
       "      <td>0</td>\n",
       "      <td>0.018799</td>\n",
       "      <td>0.674797</td>\n",
       "      <td>0.791424</td>\n",
       "      <td>0.997452</td>\n",
       "      <td>0.992000</td>\n",
       "      <td>0.351076</td>\n",
       "      <td>0.518611</td>\n",
       "      <td>0.859333</td>\n",
       "      <td>0.993056</td>\n",
       "      <td>0.734275</td>\n",
       "      <td>0.844280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>None</td>\n",
       "      <td>AdaBoostClassifier()</td>\n",
       "      <td>{\"algorithm\": \"SAMME.R\", \"base_estimator\": nul...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[\"w2v_cosine\", \"w2v_euclidean\", \"w2v_manhattan...</td>\n",
       "      <td>9977</td>\n",
       "      <td>451680</td>\n",
       "      <td>0</td>\n",
       "      <td>0.032657</td>\n",
       "      <td>0.827516</td>\n",
       "      <td>0.854278</td>\n",
       "      <td>0.997219</td>\n",
       "      <td>0.992278</td>\n",
       "      <td>0.291053</td>\n",
       "      <td>0.450088</td>\n",
       "      <td>0.718667</td>\n",
       "      <td>0.991667</td>\n",
       "      <td>0.532438</td>\n",
       "      <td>0.692868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>None</td>\n",
       "      <td>GaussianNB()</td>\n",
       "      <td>{\"priors\": null, \"var_smoothing\": 1e-09}</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[\"pca_comp_0\", \"pca_comp_1\", \"pca_comp_2\", \"pc...</td>\n",
       "      <td>9977</td>\n",
       "      <td>451680</td>\n",
       "      <td>15</td>\n",
       "      <td>0.021862</td>\n",
       "      <td>0.643888</td>\n",
       "      <td>0.802892</td>\n",
       "      <td>0.997124</td>\n",
       "      <td>0.804433</td>\n",
       "      <td>0.349377</td>\n",
       "      <td>0.487169</td>\n",
       "      <td>0.801333</td>\n",
       "      <td>0.805436</td>\n",
       "      <td>0.776552</td>\n",
       "      <td>0.790730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>None</td>\n",
       "      <td>DecisionTreeClassifier()</td>\n",
       "      <td>{\"ccp_alpha\": 0.0, \"class_weight\": null, \"crit...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[\"w2v_cosine\", \"w2v_euclidean\", \"w2v_manhattan...</td>\n",
       "      <td>9977</td>\n",
       "      <td>451680</td>\n",
       "      <td>0</td>\n",
       "      <td>0.002455</td>\n",
       "      <td>0.289354</td>\n",
       "      <td>0.568193</td>\n",
       "      <td>0.996387</td>\n",
       "      <td>0.563327</td>\n",
       "      <td>0.337486</td>\n",
       "      <td>0.422096</td>\n",
       "      <td>0.712000</td>\n",
       "      <td>0.541578</td>\n",
       "      <td>0.996078</td>\n",
       "      <td>0.701657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>None</td>\n",
       "      <td>AdaBoostClassifier()</td>\n",
       "      <td>{\"algorithm\": \"SAMME.R\", \"base_estimator\": nul...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[\"pca_comp_0\", \"pca_comp_1\", \"pca_comp_2\", \"pc...</td>\n",
       "      <td>9977</td>\n",
       "      <td>451680</td>\n",
       "      <td>15</td>\n",
       "      <td>0.032700</td>\n",
       "      <td>0.724680</td>\n",
       "      <td>0.808507</td>\n",
       "      <td>0.997248</td>\n",
       "      <td>0.994329</td>\n",
       "      <td>0.297848</td>\n",
       "      <td>0.458388</td>\n",
       "      <td>0.792667</td>\n",
       "      <td>0.993988</td>\n",
       "      <td>0.616915</td>\n",
       "      <td>0.761320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>None</td>\n",
       "      <td>AdaBoostClassifier()</td>\n",
       "      <td>{\"algorithm\": \"SAMME.R\", \"base_estimator\": nul...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[\"pca_comp_0\", \"pca_comp_1\", \"pca_comp_2\", \"pc...</td>\n",
       "      <td>9977</td>\n",
       "      <td>451680</td>\n",
       "      <td>10</td>\n",
       "      <td>0.038329</td>\n",
       "      <td>0.860752</td>\n",
       "      <td>0.855152</td>\n",
       "      <td>0.997345</td>\n",
       "      <td>0.994764</td>\n",
       "      <td>0.322763</td>\n",
       "      <td>0.487388</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.994434</td>\n",
       "      <td>0.590308</td>\n",
       "      <td>0.740843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>None</td>\n",
       "      <td>GaussianNB()</td>\n",
       "      <td>{\"priors\": null, \"var_smoothing\": 1e-09}</td>\n",
       "      <td>RankNet(\\n  (model): Sequential(\\n    (0): Lin...</td>\n",
       "      <td>50</td>\n",
       "      <td>[\"w2v_cosine\", \"w2v_euclidean\", \"w2v_manhattan...</td>\n",
       "      <td>9977</td>\n",
       "      <td>451680</td>\n",
       "      <td>0</td>\n",
       "      <td>0.031954</td>\n",
       "      <td>0.878146</td>\n",
       "      <td>0.883158</td>\n",
       "      <td>0.997173</td>\n",
       "      <td>0.996317</td>\n",
       "      <td>0.297907</td>\n",
       "      <td>0.458669</td>\n",
       "      <td>0.720667</td>\n",
       "      <td>0.996161</td>\n",
       "      <td>0.554487</td>\n",
       "      <td>0.712423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>None</td>\n",
       "      <td>GaussianNB()</td>\n",
       "      <td>{\"priors\": null, \"var_smoothing\": 1e-09}</td>\n",
       "      <td>RankNet(\\n  (model): Sequential(\\n    (0): Lin...</td>\n",
       "      <td>100</td>\n",
       "      <td>[\"w2v_cosine\", \"w2v_euclidean\", \"w2v_manhattan...</td>\n",
       "      <td>9977</td>\n",
       "      <td>451680</td>\n",
       "      <td>0</td>\n",
       "      <td>0.031436</td>\n",
       "      <td>0.881072</td>\n",
       "      <td>0.885602</td>\n",
       "      <td>0.997173</td>\n",
       "      <td>0.996627</td>\n",
       "      <td>0.316720</td>\n",
       "      <td>0.480683</td>\n",
       "      <td>0.724000</td>\n",
       "      <td>0.996364</td>\n",
       "      <td>0.570833</td>\n",
       "      <td>0.725828</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    name                     model  \\\n",
       "0   None      LogisticRegression()   \n",
       "1   None      LogisticRegression()   \n",
       "2   None      LogisticRegression()   \n",
       "3   None      LogisticRegression()   \n",
       "4   None              GaussianNB()   \n",
       "5   None              GaussianNB()   \n",
       "6   None           MLPClassifier()   \n",
       "7   None  DecisionTreeClassifier()   \n",
       "8   None  RandomForestClassifier()   \n",
       "9   None      AdaBoostClassifier()   \n",
       "10  None              GaussianNB()   \n",
       "11  None  DecisionTreeClassifier()   \n",
       "12  None      AdaBoostClassifier()   \n",
       "13  None      AdaBoostClassifier()   \n",
       "14  None              GaussianNB()   \n",
       "15  None              GaussianNB()   \n",
       "\n",
       "                                      hyperparameters  \\\n",
       "0   {\"C\": 1.0, \"class_weight\": null, \"dual\": false...   \n",
       "1   {\"C\": 1.0, \"class_weight\": null, \"dual\": false...   \n",
       "2   {\"C\": 1.0, \"class_weight\": null, \"dual\": false...   \n",
       "3   {\"C\": 1.0, \"class_weight\": null, \"dual\": false...   \n",
       "4            {\"priors\": null, \"var_smoothing\": 1e-09}   \n",
       "5            {\"priors\": null, \"var_smoothing\": 1e-09}   \n",
       "6   {\"activation\": \"relu\", \"alpha\": 0.0001, \"batch...   \n",
       "7   {\"ccp_alpha\": 0.0, \"class_weight\": null, \"crit...   \n",
       "8   {\"bootstrap\": true, \"ccp_alpha\": 0.0, \"class_w...   \n",
       "9   {\"algorithm\": \"SAMME.R\", \"base_estimator\": nul...   \n",
       "10           {\"priors\": null, \"var_smoothing\": 1e-09}   \n",
       "11  {\"ccp_alpha\": 0.0, \"class_weight\": null, \"crit...   \n",
       "12  {\"algorithm\": \"SAMME.R\", \"base_estimator\": nul...   \n",
       "13  {\"algorithm\": \"SAMME.R\", \"base_estimator\": nul...   \n",
       "14           {\"priors\": null, \"var_smoothing\": 1e-09}   \n",
       "15           {\"priors\": null, \"var_smoothing\": 1e-09}   \n",
       "\n",
       "                                       pairwise_model pairwise_k  \\\n",
       "0                                                None       None   \n",
       "1                                                None       None   \n",
       "2                                                None       None   \n",
       "3                                                None       None   \n",
       "4                                                None       None   \n",
       "5                                                None       None   \n",
       "6                                                None       None   \n",
       "7                                                None       None   \n",
       "8                                                None       None   \n",
       "9                                                None       None   \n",
       "10                                               None       None   \n",
       "11                                               None       None   \n",
       "12                                               None       None   \n",
       "13                                               None       None   \n",
       "14  RankNet(\\n  (model): Sequential(\\n    (0): Lin...         50   \n",
       "15  RankNet(\\n  (model): Sequential(\\n    (0): Lin...        100   \n",
       "\n",
       "                                             features  sampling_training  \\\n",
       "0   [\"w2v_cosine\", \"w2v_euclidean\", \"w2v_manhattan...               9977   \n",
       "1   [\"pca_comp_0\", \"pca_comp_1\", \"pca_comp_2\", \"pc...               9977   \n",
       "2   [\"pca_comp_0\", \"pca_comp_1\", \"pca_comp_2\", \"pc...               9977   \n",
       "3   [\"pca_comp_0\", \"pca_comp_1\", \"pca_comp_2\", \"pc...               9977   \n",
       "4   [\"w2v_cosine\", \"w2v_euclidean\", \"w2v_manhattan...               9977   \n",
       "5   [\"pca_comp_0\", \"pca_comp_1\", \"pca_comp_2\", \"pc...               9977   \n",
       "6   [\"w2v_cosine\", \"w2v_euclidean\", \"w2v_manhattan...               9977   \n",
       "7   [\"w2v_cosine\", \"w2v_euclidean\", \"w2v_manhattan...               9977   \n",
       "8   [\"w2v_cosine\", \"w2v_euclidean\", \"w2v_manhattan...               9977   \n",
       "9   [\"w2v_cosine\", \"w2v_euclidean\", \"w2v_manhattan...               9977   \n",
       "10  [\"pca_comp_0\", \"pca_comp_1\", \"pca_comp_2\", \"pc...               9977   \n",
       "11  [\"w2v_cosine\", \"w2v_euclidean\", \"w2v_manhattan...               9977   \n",
       "12  [\"pca_comp_0\", \"pca_comp_1\", \"pca_comp_2\", \"pc...               9977   \n",
       "13  [\"pca_comp_0\", \"pca_comp_1\", \"pca_comp_2\", \"pc...               9977   \n",
       "14  [\"w2v_cosine\", \"w2v_euclidean\", \"w2v_manhattan...               9977   \n",
       "15  [\"w2v_cosine\", \"w2v_euclidean\", \"w2v_manhattan...               9977   \n",
       "\n",
       "    sampling_test  pca       MRR       MAP      nDCG  accuracy  precision  \\\n",
       "0          451680    0  0.038060  0.865090  0.859446  0.997135   0.993724   \n",
       "1          451680    5  0.037385  0.884383  0.875099  0.996925   0.994751   \n",
       "2          451680   20  0.039170  0.869707  0.867060  0.997190   0.994036   \n",
       "3          451680   25  0.038758  0.870135  0.866985  0.997106   0.997831   \n",
       "4          451680    0  0.036072  0.874882  0.875631  0.997173   0.995943   \n",
       "5          451680   20  0.021907  0.644065  0.800962  0.997210   0.890432   \n",
       "6          451680    0  0.033422  0.763474  0.840705  0.997140   0.972112   \n",
       "7          451680    0  0.008408  0.287001  0.547333  0.996444   0.576775   \n",
       "8          451680    0  0.018799  0.674797  0.791424  0.997452   0.992000   \n",
       "9          451680    0  0.032657  0.827516  0.854278  0.997219   0.992278   \n",
       "10         451680   15  0.021862  0.643888  0.802892  0.997124   0.804433   \n",
       "11         451680    0  0.002455  0.289354  0.568193  0.996387   0.563327   \n",
       "12         451680   15  0.032700  0.724680  0.808507  0.997248   0.994329   \n",
       "13         451680   10  0.038329  0.860752  0.855152  0.997345   0.994764   \n",
       "14         451680    0  0.031954  0.878146  0.883158  0.997173   0.996317   \n",
       "15         451680    0  0.031436  0.881072  0.885602  0.997173   0.996627   \n",
       "\n",
       "      recall        f1  accuracy@50  precision@50  recall@50     f1@50  \n",
       "0   0.268969  0.423351     0.690667      0.993289   0.490608  0.656805  \n",
       "1   0.214609  0.353051     0.642000      0.994751   0.414661  0.585328  \n",
       "2   0.283126  0.440723     0.712000      0.993711   0.524917  0.686957  \n",
       "3   0.260476  0.413112     0.685333      0.997701   0.479558  0.647761  \n",
       "4   0.278029  0.434706     0.718000      0.995851   0.532741  0.694143  \n",
       "5   0.326727  0.478045     0.824000      0.886288   0.730028  0.800604  \n",
       "6   0.276331  0.430335     0.742000      0.971014   0.557007  0.707925  \n",
       "7   0.340317  0.428063     0.729333      0.558342   0.998051  0.716084  \n",
       "8   0.351076  0.518611     0.859333      0.993056   0.734275  0.844280  \n",
       "9   0.291053  0.450088     0.718667      0.991667   0.532438  0.692868  \n",
       "10  0.349377  0.487169     0.801333      0.805436   0.776552  0.790730  \n",
       "11  0.337486  0.422096     0.712000      0.541578   0.996078  0.701657  \n",
       "12  0.297848  0.458388     0.792667      0.993988   0.616915  0.761320  \n",
       "13  0.322763  0.487388     0.750000      0.994434   0.590308  0.740843  \n",
       "14  0.297907  0.458669     0.720667      0.996161   0.554487  0.712423  \n",
       "15  0.316720  0.480683     0.724000      0.996364   0.570833  0.725828  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from src.utils.utils import load\n",
    "\n",
    "results = load('data/results/results.pkl')\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "15e7f6431c3efcc25ffb982d882caeeee1a07ee55caab724c679dfe0992cca8b"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
